{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Machine Learning & Statistical Modeling\n",
    "\n",
    "## Objectives:\n",
    "1. For each zipcode, fit a linear regression model that predicts total claims\n",
    "2. Develop ML model that predicts optimal premium values\n",
    "3. Report on important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data for machine learning...\n",
      "Dataset shape: (618176, 52)\n",
      "Rows: 618,176\n",
      "Columns: 52\n",
      "\n",
      "Target variables:\n",
      "TotalPremium mean: R100.20\n",
      "TotalClaims mean: R100.41\n",
      "SumInsured mean: R609826.86\n",
      "\n",
      "Unique zipcodes: 858\n",
      "Zipcodes with at least 50 policies: 746\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and prepare data for machine learning\n",
    "print(\"Loading and preparing data for machine learning...\")\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/raw/insurance_data.txt', delimiter='|', low_memory=False)\n",
    "\n",
    "# Convert key columns to numeric\n",
    "df['TotalPremium'] = pd.to_numeric(df['TotalPremium'], errors='coerce')\n",
    "df['TotalClaims'] = pd.to_numeric(df['TotalClaims'], errors='coerce')\n",
    "df['SumInsured'] = pd.to_numeric(df['SumInsured'], errors='coerce')\n",
    "\n",
    "# Filter out zero premiums and create working dataset\n",
    "df_ml = df[df['TotalPremium'] > 0].copy()\n",
    "\n",
    "# Basic info\n",
    "print(f\"Dataset shape: {df_ml.shape}\")\n",
    "print(f\"Rows: {df_ml.shape[0]:,}\")\n",
    "print(f\"Columns: {df_ml.shape[1]}\")\n",
    "print()\n",
    "\n",
    "# Check target variables\n",
    "print(\"Target variables:\")\n",
    "print(f\"TotalPremium mean: R{df_ml['TotalPremium'].mean():.2f}\")\n",
    "print(f\"TotalClaims mean: R{df_ml['TotalClaims'].mean():.2f}\")\n",
    "print(f\"SumInsured mean: R{df_ml['SumInsured'].mean():.2f}\")\n",
    "print()\n",
    "\n",
    "# Check unique zipcodes\n",
    "if 'PostalCode' in df_ml.columns:\n",
    "    print(f\"Unique zipcodes: {df_ml['PostalCode'].nunique():,}\")\n",
    "    print(f\"Zipcodes with at least 50 policies: {(df_ml['PostalCode'].value_counts() >= 50).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREPARING FEATURES FOR MACHINE LEARNING\n",
      "============================================================\n",
      "Available features: 21\n",
      "Features available for modeling:\n",
      "   1. SumInsured\n",
      "   2. ExcessSelected\n",
      "   3. CoverType\n",
      "   4. Product\n",
      "   5. TermFrequency\n",
      "   6. Province\n",
      "   7. PostalCode\n",
      "   8. Gender\n",
      "   9. MaritalStatus\n",
      "  10. AccountType\n",
      "  11. VehicleType\n",
      "  12. make\n",
      "  13. Model\n",
      "  14. RegistrationYear\n",
      "  15. Cylinders\n",
      "  16. cubiccapacity\n",
      "  17. bodytype\n",
      "  18. NumberOfDoors\n",
      "  19. AlarmImmobiliser\n",
      "  20. TrackingDevice\n",
      "  21. NewVehicle\n",
      "\n",
      "Missing values in selected features:\n",
      "  Gender                       4,621 (0.7%)\n",
      "  MaritalStatus                5,071 (0.8%)\n",
      "  AccountType                 30,734 (5.0%)\n",
      "  VehicleType                    218 (0.0%)\n",
      "  make                           218 (0.0%)\n",
      "  Model                          218 (0.0%)\n",
      "  Cylinders                      218 (0.0%)\n",
      "  cubiccapacity                  218 (0.0%)\n",
      "  bodytype                       218 (0.0%)\n",
      "  NumberOfDoors                  218 (0.0%)\n",
      "  NewVehicle                  60,634 (9.8%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Prepare features for modeling\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPARING FEATURES FOR MACHINE LEARNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select features for modeling\n",
    "# Based on domain knowledge and data availability\n",
    "potential_features = [\n",
    "    # Policy/Client features\n",
    "    'SumInsured',\n",
    "    'ExcessSelected',  # Might be categorical\n",
    "    'CoverType',\n",
    "    'Product',\n",
    "    'TermFrequency',\n",
    "    \n",
    "    # Client demographics\n",
    "    'Province',\n",
    "    'PostalCode',\n",
    "    'Gender',\n",
    "    'MaritalStatus',\n",
    "    'AccountType',\n",
    "    \n",
    "    # Vehicle features\n",
    "    'VehicleType',\n",
    "    'make',\n",
    "    'Model',\n",
    "    'RegistrationYear',\n",
    "    'Cylinders',\n",
    "    'cubiccapacity',\n",
    "    'bodytype',\n",
    "    'NumberOfDoors',\n",
    "    \n",
    "    # Risk features\n",
    "    'AlarmImmobiliser',\n",
    "    'TrackingDevice',\n",
    "    'NewVehicle',\n",
    "]\n",
    "\n",
    "# Check which features are available\n",
    "available_features = [f for f in potential_features if f in df_ml.columns]\n",
    "print(f\"Available features: {len(available_features)}\")\n",
    "print(\"Features available for modeling:\")\n",
    "for i, feat in enumerate(available_features, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "print()\n",
    "\n",
    "# Check for missing values in these features\n",
    "print(\"Missing values in selected features:\")\n",
    "missing_counts = {}\n",
    "for feat in available_features:\n",
    "    missing = df_ml[feat].isnull().sum()\n",
    "    if missing > 0:\n",
    "        missing_counts[feat] = missing\n",
    "        print(f\"  {feat:25} {missing:8,} ({missing/len(df_ml)*100:.1f}%)\")\n",
    "\n",
    "if not missing_counts:\n",
    "    print(\"  No missing values in selected features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING FINAL MODELING DATASET\n",
      "============================================================\n",
      "Selected features: ['SumInsured', 'Province', 'VehicleType', 'CoverType', 'TermFrequency', 'AlarmImmobiliser', 'PostalCode']\n",
      "Target 1: TotalClaims (for claims prediction)\n",
      "Target 2: TotalPremium (for premium prediction)\n",
      "\n",
      "Data types of selected features:\n",
      "  SumInsured           float64    Unique: N/A\n",
      "  Province             object     Unique: 9\n",
      "  VehicleType          object     Unique: 5\n",
      "  CoverType            object     Unique: 21\n",
      "  TermFrequency        object     Unique: 2\n",
      "  AlarmImmobiliser     object     Unique: 2\n",
      "  PostalCode           int64      Unique: N/A\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create final modeling dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATING FINAL MODELING DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a copy for modeling\n",
    "df_model = df_ml.copy()\n",
    "\n",
    "# 1. Select features (start with a manageable set)\n",
    "selected_features = [\n",
    "    'SumInsured',           # Important: higher sum insured = higher risk\n",
    "    'Province',             # Geographic risk\n",
    "    'VehicleType',          # Type of vehicle\n",
    "    'CoverType',            # Type of coverage\n",
    "    'TermFrequency',        # Payment frequency\n",
    "    'AlarmImmobiliser',     # Security feature\n",
    "]\n",
    "\n",
    "# Also include zipcode for grouping\n",
    "if 'PostalCode' in df_model.columns:\n",
    "    selected_features.append('PostalCode')\n",
    "\n",
    "# Target variables\n",
    "target_claims = 'TotalClaims'\n",
    "target_premium = 'TotalPremium'\n",
    "\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "print(f\"Target 1: {target_claims} (for claims prediction)\")\n",
    "print(f\"Target 2: {target_premium} (for premium prediction)\")\n",
    "print()\n",
    "\n",
    "# Check data types\n",
    "print(\"Data types of selected features:\")\n",
    "for feat in selected_features:\n",
    "    if feat in df_model.columns:\n",
    "        dtype = df_model[feat].dtype\n",
    "        unique = df_model[feat].nunique() if dtype == 'object' else 'N/A'\n",
    "        print(f\"  {feat:20} {str(dtype):10} Unique: {unique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OBJECTIVE 1: LINEAR REGRESSION FOR TOTAL CLAIMS\n",
      "============================================================\n",
      "Starting with numerical features only...\n",
      "\n",
      "1. Correlation analysis:\n",
      "   SumInsured           Correlation with Claims: -0.0063\n",
      "\n",
      "2. Data split:\n",
      "   Training samples: 494,540\n",
      "   Test samples: 123,636\n",
      "\n",
      "3. Model Performance:\n",
      "   Mean Squared Error (MSE): 6,522,068.32\n",
      "   Root Mean Squared Error (RMSE): 2,553.83\n",
      "   Mean Absolute Error (MAE): 193.70\n",
      "   R-squared (R²): 0.0001\n",
      "\n",
      "4. Model Coefficients:\n",
      "   SumInsured: -0.000012\n",
      "   Intercept: 109.51\n",
      "\n",
      "5. Interpretation:\n",
      "   • R² = 0.0001 means the model explains 0.0% of variance in claims\n",
      "   • RMSE = R2553.83 means average prediction error is R2553.83\n",
      "   • WARNING: Very low R². Need more/better features.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Simple Linear Regression for TotalClaims\n",
    "print(\"=\" * 60)\n",
    "print(\"OBJECTIVE 1: LINEAR REGRESSION FOR TOTAL CLAIMS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For linear regression, we need to handle categorical variables\n",
    "# Let's start with just numerical features for simplicity\n",
    "print(\"Starting with numerical features only...\")\n",
    "\n",
    "# Select numerical features for initial model\n",
    "numerical_features = ['SumInsured']  # Start simple\n",
    "\n",
    "# Check correlation with TotalClaims\n",
    "print(\"\\n1. Correlation analysis:\")\n",
    "for feat in numerical_features:\n",
    "    if feat in df_model.columns:\n",
    "        corr = df_model[feat].corr(df_model['TotalClaims'])\n",
    "        print(f\"   {feat:20} Correlation with Claims: {corr:.4f}\")\n",
    "\n",
    "# Prepare data\n",
    "X = df_model[numerical_features].fillna(df_model[numerical_features].mean())\n",
    "y = df_model['TotalClaims']\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\n2. Data split:\")\n",
    "print(f\"   Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "# Train linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n3. Model Performance:\")\n",
    "print(f\"   Mean Squared Error (MSE): {mse:,.2f}\")\n",
    "print(f\"   Root Mean Squared Error (RMSE): {rmse:,.2f}\")\n",
    "print(f\"   Mean Absolute Error (MAE): {mae:,.2f}\")\n",
    "print(f\"   R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "print(f\"\\n4. Model Coefficients:\")\n",
    "for i, feat in enumerate(numerical_features):\n",
    "    print(f\"   {feat}: {lr_model.coef_[i]:.6f}\")\n",
    "print(f\"   Intercept: {lr_model.intercept_:.2f}\")\n",
    "\n",
    "print(f\"\\n5. Interpretation:\")\n",
    "print(f\"   • R² = {r2:.4f} means the model explains {r2*100:.1f}% of variance in claims\")\n",
    "print(f\"   • RMSE = R{rmse:.2f} means average prediction error is R{rmse:.2f}\")\n",
    "if r2 < 0.1:\n",
    "    print(f\"   • WARNING: Very low R². Need more/better features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADDING CATEGORICAL FEATURES WITH ONE-HOT ENCODING\n",
      "============================================================\n",
      "Features for enhanced model: ['SumInsured', 'Province', 'VehicleType', 'CoverType']\n",
      "\n",
      "Training enhanced linear regression model...\n",
      "\n",
      "Enhanced Model Performance:\n",
      "   R-squared (R²): 0.0059 (explains 0.6% of variance)\n",
      "   RMSE: R2546.34\n",
      "   MAE: R209.93\n",
      "\n",
      "Comparison with simple model:\n",
      "   Simple model R²: 0.0059\n",
      "   Enhanced model R²: 0.0059\n",
      "   Improvement: 0.0000\n",
      "\n",
      "Top 10 most important features (by absolute coefficient):\n",
      "   CoverType_Standalone passenger liability Coef: -8900.04\n",
      "   CoverType_Passenger Liability            Coef: -8895.80\n",
      "   CoverType_Third Party Only               Coef: -1828.63\n",
      "   CoverType_Own Damage                     Coef:  1642.52\n",
      "   VehicleType_nan                          Coef:  1460.32\n",
      "   CoverType_Income Protector               Coef:  1219.76\n",
      "   CoverType_Trailer                        Coef:  1196.95\n",
      "   CoverType_Windscreen                     Coef:  1188.67\n",
      "   CoverType_Roadside Assistance            Coef:  1166.06\n",
      "   CoverType_Cash Takings                   Coef:  1163.68\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Adding Categorical Features with One-Hot Encoding\n",
    "print(\"=\" * 60)\n",
    "print(\"ADDING CATEGORICAL FEATURES WITH ONE-HOT ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select categorical features to add\n",
    "categorical_features = ['Province', 'VehicleType', 'CoverType']\n",
    "\n",
    "# Create new feature set\n",
    "all_features = numerical_features + categorical_features\n",
    "\n",
    "print(f\"Features for enhanced model: {all_features}\")\n",
    "print()\n",
    "\n",
    "# Prepare data with one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = numerical_features\n",
    "categorical_cols = categorical_features\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Create pipeline with preprocessing and linear regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Split data\n",
    "X = df_model[all_features]\n",
    "y = df_model['TotalClaims']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "print(\"Training enhanced linear regression model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nEnhanced Model Performance:\")\n",
    "print(f\"   R-squared (R²): {r2:.4f} (explains {r2*100:.1f}% of variance)\")\n",
    "print(f\"   RMSE: R{rmse:.2f}\")\n",
    "print(f\"   MAE: R{mae:.2f}\")\n",
    "\n",
    "# Compare with simple model\n",
    "print(f\"\\nComparison with simple model:\")\n",
    "print(f\"   Simple model R²: {r2:.4f}\")\n",
    "print(f\"   Enhanced model R²: {r2:.4f}\")\n",
    "print(f\"   Improvement: {(r2 - r2):.4f}\")\n",
    "\n",
    "# Get feature importance (for linear model, coefficients)\n",
    "try:\n",
    "    # Extract feature names after one-hot encoding\n",
    "    cat_encoder = pipeline.named_steps['preprocessor'].named_transformers_['cat']\n",
    "    cat_feature_names = cat_encoder.get_feature_names_out(categorical_cols)\n",
    "    all_feature_names = numerical_cols + list(cat_feature_names)\n",
    "    \n",
    "    # Get coefficients\n",
    "    coefficients = pipeline.named_steps['regressor'].coef_\n",
    "    \n",
    "    print(f\"\\nTop 10 most important features (by absolute coefficient):\")\n",
    "    feat_importance = pd.DataFrame({\n",
    "        'Feature': all_feature_names,\n",
    "        'Coefficient': coefficients\n",
    "    })\n",
    "    feat_importance['Abs_Coefficient'] = np.abs(feat_importance['Coefficient'])\n",
    "    feat_importance = feat_importance.sort_values('Abs_Coefficient', ascending=False).head(10)\n",
    "    \n",
    "    for _, row in feat_importance.iterrows():\n",
    "        print(f\"   {row['Feature'][:40]:40} Coef: {row['Coefficient']:8.2f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not extract feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS: WHY LINEAR REGRESSION FAILS FOR CLAIMS PREDICTION\n",
      "======================================================================\n",
      "\n",
      "KEY FINDING FROM DATA:\n",
      "1. Claim Distribution:\n",
      "   • Policies with ZERO claims: 615,533 (99.6%)\n",
      "   • Policies WITH claims: 2,641 (0.4%)\n",
      "   • Average claim amount (when claims occur): R23510.32\n",
      "\n",
      "2. Statistical Challenge:\n",
      "   • Linear regression assumes normal distribution\n",
      "   • Claims data is HIGHLY SKEWED: mostly zeros, few huge values\n",
      "   • This violates linear regression assumptions\n",
      "\n",
      "3. Business Implication:\n",
      "   • Traditional pricing models may fail for this portfolio\n",
      "   • Need specialized models for rare, catastrophic events\n",
      "   • Consider: Zero-inflated models, Poisson regression, etc.\n",
      "\n",
      "RECOMMENDED APPROACHES:\n",
      "1. Two-stage modeling:\n",
      "   • Stage 1: Predict IF a claim will occur (classification)\n",
      "   • Stage 2: Predict HOW MUCH claim will be (regression on claims>0)\n",
      "\n",
      "2. Alternative models:\n",
      "   • Random Forest / Gradient Boosting (handle skewed data better)\n",
      "   • Zero-inflated Poisson regression\n",
      "   • Quantile regression (predict worst-case scenarios)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Analyzing why linear regression fails\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS: WHY LINEAR REGRESSION FAILS FOR CLAIMS PREDICTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKEY FINDING FROM DATA:\")\n",
    "print(\"1. Claim Distribution:\")\n",
    "print(f\"   • Policies with ZERO claims: {(df_model['TotalClaims'] == 0).sum():,} (99.6%)\")\n",
    "print(f\"   • Policies WITH claims: {(df_model['TotalClaims'] > 0).sum():,} (0.4%)\")\n",
    "print(f\"   • Average claim amount (when claims occur): R{df_model[df_model['TotalClaims'] > 0]['TotalClaims'].mean():.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"2. Statistical Challenge:\")\n",
    "print(\"   • Linear regression assumes normal distribution\")\n",
    "print(\"   • Claims data is HIGHLY SKEWED: mostly zeros, few huge values\")\n",
    "print(\"   • This violates linear regression assumptions\")\n",
    "print()\n",
    "\n",
    "print(\"3. Business Implication:\")\n",
    "print(\"   • Traditional pricing models may fail for this portfolio\")\n",
    "print(\"   • Need specialized models for rare, catastrophic events\")\n",
    "print(\"   • Consider: Zero-inflated models, Poisson regression, etc.\")\n",
    "print()\n",
    "\n",
    "print(\"RECOMMENDED APPROACHES:\")\n",
    "print(\"1. Two-stage modeling:\")\n",
    "print(\"   • Stage 1: Predict IF a claim will occur (classification)\")\n",
    "print(\"   • Stage 2: Predict HOW MUCH claim will be (regression on claims>0)\")\n",
    "print()\n",
    "print(\"2. Alternative models:\")\n",
    "print(\"   • Random Forest / Gradient Boosting (handle skewed data better)\")\n",
    "print(\"   • Zero-inflated Poisson regression\")\n",
    "print(\"   • Quantile regression (predict worst-case scenarios)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RANDOM FOREST REGRESSOR (Handles skewed data better)\n",
      "============================================================\n",
      "Features for Random Forest: ['SumInsured', 'Province', 'VehicleType', 'CoverType', 'TermFrequency', 'AlarmImmobiliser', 'PostalCode']\n",
      "\n",
      "Training samples: 494,540\n",
      "Test samples: 123,636\n",
      "Claims in training set: 2,113 (0.43%)\n",
      "Claims in test set: 528 (0.43%)\n",
      "\n",
      "Training Random Forest (this may take a minute)...\n",
      "\n",
      "Random Forest Performance:\n",
      "   R-squared (R²): 0.0234 (explains 2.3% of variance)\n",
      "   RMSE: R2896.79\n",
      "   MAE: R190.77\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "   SumInsured                0.5342\n",
      "   PostalCode                0.2814\n",
      "   CoverType                 0.1509\n",
      "   VehicleType               0.0212\n",
      "   Province                  0.0123\n",
      "   AlarmImmobiliser          0.0000\n",
      "   TermFrequency             0.0000\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Random Forest for Claims Prediction\n",
    "print(\"=\" * 60)\n",
    "print(\"RANDOM FOREST REGRESSOR (Handles skewed data better)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare data for Random Forest (handles categorical natively)\n",
    "# Let's encode categorical variables\n",
    "df_rf = df_model.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_cols = ['Province', 'VehicleType', 'CoverType', 'TermFrequency', 'AlarmImmobiliser']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_rf.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Handle NaN values\n",
    "        df_rf[col] = df_rf[col].fillna('Missing')\n",
    "        df_rf[col] = le.fit_transform(df_rf[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Features for Random Forest\n",
    "rf_features = ['SumInsured'] + categorical_cols\n",
    "if 'PostalCode' in df_rf.columns:\n",
    "    rf_features.append('PostalCode')\n",
    "\n",
    "print(f\"Features for Random Forest: {rf_features}\")\n",
    "print()\n",
    "\n",
    "# Prepare X and y\n",
    "X = df_rf[rf_features].fillna(0)\n",
    "y = df_rf['TotalClaims']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=(y > 0))\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Test samples: {X_test.shape[0]:,}\")\n",
    "print(f\"Claims in training set: {(y_train > 0).sum():,} ({((y_train > 0).sum()/len(y_train))*100:.2f}%)\")\n",
    "print(f\"Claims in test set: {(y_test > 0).sum():,} ({((y_test > 0).sum()/len(y_test))*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Train Random Forest (with limited trees for speed)\n",
    "print(\"Training Random Forest (this may take a minute)...\")\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,  # Reduced for speed\n",
    "    max_depth=10,\n",
    "    min_samples_split=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"\\nRandom Forest Performance:\")\n",
    "print(f\"   R-squared (R²): {r2_rf:.4f} (explains {r2_rf*100:.1f}% of variance)\")\n",
    "print(f\"   RMSE: R{rmse_rf:.2f}\")\n",
    "print(f\"   MAE: R{mae_rf:.2f}\")\n",
    "print()\n",
    "\n",
    "# Feature importance\n",
    "print(\"Top 10 Feature Importances:\")\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': rf_features,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "for i, row in feature_importance_df.head(10).iterrows():\n",
    "    print(f\"   {row['Feature']:25} {row['Importance']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OBJECTIVE 1: LINEAR REGRESSION FOR EACH ZIPCODE\n",
      "======================================================================\n",
      "Demonstrating methodology for top 5 zipcodes by policy count\n",
      "\n",
      "\n",
      "========================================\n",
      "ZIPCODE: 2000\n",
      "========================================\n",
      "   Policies: 90,934\n",
      "   Policies with claims: 471 (0.5%)\n",
      "   Linear Regression R²: 0.0001\n",
      "   Coefficient (SumInsured): -0.00001835\n",
      "   Intercept: 110.24\n",
      "\n",
      "========================================\n",
      "ZIPCODE: 122\n",
      "========================================\n",
      "   Policies: 27,898\n",
      "   Policies with claims: 190 (0.7%)\n",
      "   Linear Regression R²: 0.0001\n",
      "   Coefficient (SumInsured): -0.00001897\n",
      "   Intercept: 130.29\n",
      "\n",
      "========================================\n",
      "ZIPCODE: 299\n",
      "========================================\n",
      "   Policies: 16,731\n",
      "   Policies with claims: 65 (0.4%)\n",
      "   Linear Regression R²: 0.0001\n",
      "   Coefficient (SumInsured): -0.00000852\n",
      "   Intercept: 54.53\n",
      "\n",
      "========================================\n",
      "ZIPCODE: 7784\n",
      "========================================\n",
      "   Policies: 13,560\n",
      "   Policies with claims: 45 (0.3%)\n",
      "   Linear Regression R²: 0.0000\n",
      "   Coefficient (SumInsured): -0.00001750\n",
      "   Intercept: 132.47\n",
      "\n",
      "========================================\n",
      "ZIPCODE: 7405\n",
      "========================================\n",
      "   Policies: 10,731\n",
      "   Policies with claims: 29 (0.3%)\n",
      "   Linear Regression R²: 0.0002\n",
      "   Coefficient (SumInsured): -0.00001118\n",
      "   Intercept: 64.41\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: Linear Regression per Zipcode\n",
      "======================================================================\n",
      " Zipcode  Policies  ClaimsPct       R2  Coefficient  Intercept\n",
      "    2000     90934   0.517958 0.000131    -0.000018 110.240959\n",
      "     122     27898   0.681052 0.000079    -0.000019 130.293217\n",
      "     299     16731   0.388500 0.000073    -0.000009  54.532138\n",
      "    7784     13560   0.331858 0.000043    -0.000018 132.467595\n",
      "    7405     10731   0.270245 0.000170    -0.000011  64.413682\n",
      "\n",
      "Key Insights:\n",
      "1. All zipcodes have very low claim frequency (< 0.5%)\n",
      "2. R² values are very low (near zero)\n",
      "3. SumInsured has minimal predictive power for claims\n",
      "4. Need more sophisticated modeling approach\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Linear Regression per Zipcode (for top 5 zipcodes)\n",
    "print(\"=\" * 70)\n",
    "print(\"OBJECTIVE 1: LINEAR REGRESSION FOR EACH ZIPCODE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Demonstrating methodology for top 5 zipcodes by policy count\")\n",
    "print()\n",
    "\n",
    "# Get top 5 zipcodes\n",
    "top_zipcodes = df_model['PostalCode'].value_counts().head(5).index\n",
    "\n",
    "results = []\n",
    "\n",
    "for zipcode in top_zipcodes:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"ZIPCODE: {zipcode}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Filter data for this zipcode\n",
    "    zip_data = df_model[df_model['PostalCode'] == zipcode].copy()\n",
    "    \n",
    "    if len(zip_data) < 50:\n",
    "        print(f\"   Skipping - only {len(zip_data)} policies\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"   Policies: {len(zip_data):,}\")\n",
    "    print(f\"   Policies with claims: {(zip_data['TotalClaims'] > 0).sum():,} ({(zip_data['TotalClaims'] > 0).sum()/len(zip_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Simple linear regression: SumInsured -> TotalClaims\n",
    "    X_zip = zip_data[['SumInsured']].fillna(zip_data['SumInsured'].mean())\n",
    "    y_zip = zip_data['TotalClaims']\n",
    "    \n",
    "    if len(X_zip) > 10:\n",
    "        # Train model\n",
    "        lr_zip = LinearRegression()\n",
    "        lr_zip.fit(X_zip, y_zip)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_zip = lr_zip.predict(X_zip)\n",
    "        \n",
    "        # Calculate R²\n",
    "        r2_zip = r2_score(y_zip, y_pred_zip)\n",
    "        \n",
    "        print(f\"   Linear Regression R²: {r2_zip:.4f}\")\n",
    "        print(f\"   Coefficient (SumInsured): {lr_zip.coef_[0]:.8f}\")\n",
    "        print(f\"   Intercept: {lr_zip.intercept_:.2f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'Zipcode': zipcode,\n",
    "            'Policies': len(zip_data),\n",
    "            'ClaimsPct': (zip_data['TotalClaims'] > 0).sum()/len(zip_data)*100,\n",
    "            'R2': r2_zip,\n",
    "            'Coefficient': lr_zip.coef_[0],\n",
    "            'Intercept': lr_zip.intercept_\n",
    "        })\n",
    "    else:\n",
    "        print(\"   Insufficient data for regression\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY: Linear Regression per Zipcode\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nKey Insights:\")\n",
    "    print(f\"1. All zipcodes have very low claim frequency (< 0.5%)\")\n",
    "    print(f\"2. R² values are very low (near zero)\")\n",
    "    print(f\"3. SumInsured has minimal predictive power for claims\")\n",
    "    print(f\"4. Need more sophisticated modeling approach\")\n",
    "else:\n",
    "    print(\"No valid results - insufficient data in individual zipcodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OBJECTIVE 2: MACHINE LEARNING FOR OPTIMAL PREMIUM PREDICTION\n",
      "======================================================================\n",
      "Profitable policies: 99.6%\n",
      "\n",
      "Profitable policies for training: 615,551\n",
      "Training data: 492,440 samples\n",
      "Test data: 123,111 samples\n",
      "Average premium in training: R98.56\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 (FIXED COMPLETE): Prepare data for premium prediction\n",
    "print(\"=\" * 70)\n",
    "print(\"OBJECTIVE 2: MACHINE LEARNING FOR OPTIMAL PREMIUM PREDICTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate LossRatio if not exists\n",
    "if 'LossRatio' not in df_model.columns:\n",
    "    df_model['LossRatio'] = df_model['TotalClaims'] / df_model['TotalPremium']\n",
    "    df_model['LossRatio'] = df_model['LossRatio'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Identify profitable policies\n",
    "df_model['Profitable'] = df_model['LossRatio'] < 1\n",
    "profitable_pct = df_model['Profitable'].mean() * 100\n",
    "\n",
    "print(f\"Profitable policies: {profitable_pct:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Prepare data for premium prediction\n",
    "df_premium = df_model.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['Province', 'VehicleType', 'CoverType', 'TermFrequency', 'AlarmImmobiliser']\n",
    "for col in categorical_cols:\n",
    "    if col in df_premium.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_premium[col] = df_premium[col].fillna('Missing')\n",
    "        df_premium[col] = le.fit_transform(df_premium[col].astype(str))\n",
    "\n",
    "# Features for premium prediction\n",
    "premium_features = [\n",
    "    'SumInsured',\n",
    "    'Province',\n",
    "    'VehicleType', \n",
    "    'CoverType',\n",
    "    'TermFrequency',\n",
    "    'AlarmImmobiliser',\n",
    "    'PostalCode'\n",
    "]\n",
    "\n",
    "# Filter to only profitable policies for training\n",
    "df_profitable = df_premium[df_premium['Profitable']].copy()\n",
    "print(f\"Profitable policies for training: {len(df_profitable):,}\")\n",
    "\n",
    "# Define X and y\n",
    "X_premium = df_profitable[premium_features].fillna(0)\n",
    "y_premium = df_profitable['TotalPremium']\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
    "    X_premium, y_premium, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training data: {X_train_p.shape[0]:,} samples\")\n",
    "print(f\"Test data: {X_test_p.shape[0]:,} samples\")\n",
    "print(f\"Average premium in training: R{y_train_p.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRADIENT BOOSTING FOR PREMIUM PREDICTION\n",
      "============================================================\n",
      "Training Gradient Boosting Regressor...\n",
      "\n",
      "Model Performance:\n",
      "   R-squared (R²): 0.4252 (explains 42.5% of variance)\n",
      "   RMSE: R250.85\n",
      "   MAE: R19.52\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 (SIMPLIFIED): Gradient Boosting for Premium Prediction\n",
    "print(\"=\" * 60)\n",
    "print(\"GRADIENT BOOSTING FOR PREMIUM PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Make sure X_train_p exists\n",
    "if 'X_train_p' not in locals():\n",
    "    print(\"ERROR: X_train_p not defined. Run cell 10 first.\")\n",
    "else:\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    \n",
    "    print(\"Training Gradient Boosting Regressor...\")\n",
    "    \n",
    "    # Train with fewer estimators for speed\n",
    "    gb_model = GradientBoostingRegressor(\n",
    "        n_estimators=50,  # Reduced for speed\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    gb_model.fit(X_train_p, y_train_p)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_gb = gb_model.predict(X_test_p)\n",
    "    \n",
    "    # Evaluate\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    mse_gb = mean_squared_error(y_test_p, y_pred_gb)\n",
    "    rmse_gb = np.sqrt(mse_gb)\n",
    "    mae_gb = mean_absolute_error(y_test_p, y_pred_gb)\n",
    "    r2_gb = r2_score(y_test_p, y_pred_gb)\n",
    "    \n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"   R-squared (R²): {r2_gb:.4f} (explains {r2_gb*100:.1f}% of variance)\")\n",
    "    print(f\"   RMSE: R{rmse_gb:.2f}\")\n",
    "    print(f\"   MAE: R{mae_gb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUGGESTING OPTIMAL PREMIUMS FOR ALL POLICIES\n",
      "======================================================================\n",
      "Predicting optimal premiums for all policies...\n",
      "\n",
      "Comparison: Current vs Optimal Premiums\n",
      "Current average premium: R100.20\n",
      "Optimal average premium: R99.61\n",
      "Average change: R-0.59\n",
      "Average % change: 2231.2%\n",
      "\n",
      "Premium Adjustment Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "Profitable Policies (615,551 policies):\n",
      "   Current premium: R98.84\n",
      "   Optimal premium: R98.68\n",
      "   Suggested change: R-0.16 (+2240.7%)\n",
      "   ✓ Suggests REDUCING premium (attract more customers)\n",
      "\n",
      "Unprofitable Policies (2,625 policies):\n",
      "   Current premium: R419.41\n",
      "   Optimal premium: R317.35\n",
      "   Suggested change: R-102.06 (+1.5%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Apply model to ALL policies to suggest optimal premiums\n",
    "print(\"=\" * 70)\n",
    "print(\"SUGGESTING OPTIMAL PREMIUMS FOR ALL POLICIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare all data for prediction\n",
    "X_all = df_premium[premium_features].fillna(0)\n",
    "\n",
    "# Predict optimal premiums using our model\n",
    "print(\"Predicting optimal premiums for all policies...\")\n",
    "optimal_premiums = gb_model.predict(X_all)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_premium['OptimalPremium'] = optimal_premiums\n",
    "\n",
    "# Compare current vs optimal\n",
    "df_premium['PremiumDifference'] = df_premium['OptimalPremium'] - df_premium['TotalPremium']\n",
    "df_premium['PremiumChangePct'] = (df_premium['PremiumDifference'] / df_premium['TotalPremium']) * 100\n",
    "\n",
    "print(f\"\\nComparison: Current vs Optimal Premiums\")\n",
    "print(f\"Current average premium: R{df_premium['TotalPremium'].mean():.2f}\")\n",
    "print(f\"Optimal average premium: R{df_premium['OptimalPremium'].mean():.2f}\")\n",
    "print(f\"Average change: R{df_premium['PremiumDifference'].mean():.2f}\")\n",
    "print(f\"Average % change: {df_premium['PremiumChangePct'].mean():.1f}%\")\n",
    "print()\n",
    "\n",
    "# Analyze by profitability\n",
    "print(\"Premium Adjustment Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for profitable in [True, False]:\n",
    "    subset = df_premium[df_premium['Profitable'] == profitable]\n",
    "    label = \"Profitable\" if profitable else \"Unprofitable\"\n",
    "    \n",
    "    avg_current = subset['TotalPremium'].mean()\n",
    "    avg_optimal = subset['OptimalPremium'].mean()\n",
    "    avg_change = subset['PremiumDifference'].mean()\n",
    "    avg_pct_change = subset['PremiumChangePct'].mean()\n",
    "    \n",
    "    print(f\"\\n{label} Policies ({len(subset):,} policies):\")\n",
    "    print(f\"   Current premium: R{avg_current:.2f}\")\n",
    "    print(f\"   Optimal premium: R{avg_optimal:.2f}\")\n",
    "    print(f\"   Suggested change: R{avg_change:+.2f} ({avg_pct_change:+.1f}%)\")\n",
    "    \n",
    "    if profitable and avg_change < 0:\n",
    "        print(f\"   ✓ Suggests REDUCING premium (attract more customers)\")\n",
    "    elif not profitable and avg_change > 0:\n",
    "        print(f\"   ✓ Suggests INCREASING premium (improve profitability)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL RECOMMENDATIONS: OPTIMAL PRICING STRATEGY\n",
      "======================================================================\n",
      "\n",
      "1. KEY FINDINGS FROM MACHINE LEARNING:\n",
      "   • Claims are RARE (0.4%) but CATASTROPHIC when they occur\n",
      "   • Traditional linear models fail (R² < 0.01)\n",
      "   • Random Forest explains 2.3% of claim variance\n",
      "   • Most important factors: SumInsured, PostalCode, CoverType\n",
      "\n",
      "2. OPTIMAL PRICING MODEL RESULTS:\n",
      "   • Gradient Boosting R²: 0.4252 (42.5% variance explained)\n",
      "   • Model trained on profitable policies only\n",
      "   • Can suggest premium adjustments for all policies\n",
      "\n",
      "3. PREMIUM ADJUSTMENT STRATEGY:\n",
      "   For PROFITABLE policies (LossRatio < 1):\n",
      "   • Consider SMALL premium REDUCTIONS\n",
      "   • Attract more customers in low-risk segments\n",
      "   • Gain market share while maintaining profitability\n",
      "\n",
      "   For UNPROFITABLE policies (LossRatio ≥ 1):\n",
      "   • Need premium INCREASES\n",
      "   • Or reconsider risk appetite for these segments\n",
      "   • Focus on zipcodes/vehicle types with worst loss ratios\n",
      "\n",
      "4. IMPLEMENTATION RECOMMENDATIONS:\n",
      "   a. PHASED APPROACH:\n",
      "      • Start with 5-10% premium adjustments\n",
      "      • Monitor impact on portfolio profitability\n",
      "      • Adjust based on actual results\n",
      "\n",
      "   b. SEGMENT-SPECIFIC STRATEGY:\n",
      "      • Different adjustments by province, zipcode, vehicle type\n",
      "      • Use model feature importances to guide strategy\n",
      "\n",
      "   c. CONTINUOUS MONITORING:\n",
      "      • Update models quarterly with new data\n",
      "      • Track actual vs predicted loss ratios\n",
      "      • Refine models based on performance\n",
      "\n",
      "5. EXPECTED BUSINESS IMPACT:\n",
      "   • Improved portfolio profitability\n",
      "   • More competitive pricing for low-risk segments\n",
      "   • Better risk selection and pricing accuracy\n",
      "   • Data-driven decision making\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Final Recommendations and Business Impact\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL RECOMMENDATIONS: OPTIMAL PRICING STRATEGY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. KEY FINDINGS FROM MACHINE LEARNING:\")\n",
    "print(\"   • Claims are RARE (0.4%) but CATASTROPHIC when they occur\")\n",
    "print(\"   • Traditional linear models fail (R² < 0.01)\")\n",
    "print(\"   • Random Forest explains 2.3% of claim variance\")\n",
    "print(\"   • Most important factors: SumInsured, PostalCode, CoverType\")\n",
    "print()\n",
    "\n",
    "print(\"2. OPTIMAL PRICING MODEL RESULTS:\")\n",
    "print(f\"   • Gradient Boosting R²: {r2_gb:.4f} ({r2_gb*100:.1f}% variance explained)\")\n",
    "print(\"   • Model trained on profitable policies only\")\n",
    "print(\"   • Can suggest premium adjustments for all policies\")\n",
    "print()\n",
    "\n",
    "print(\"3. PREMIUM ADJUSTMENT STRATEGY:\")\n",
    "print(\"   For PROFITABLE policies (LossRatio < 1):\")\n",
    "print(\"   • Consider SMALL premium REDUCTIONS\")\n",
    "print(\"   • Attract more customers in low-risk segments\")\n",
    "print(\"   • Gain market share while maintaining profitability\")\n",
    "print()\n",
    "print(\"   For UNPROFITABLE policies (LossRatio ≥ 1):\")\n",
    "print(\"   • Need premium INCREASES\")\n",
    "print(\"   • Or reconsider risk appetite for these segments\")\n",
    "print(\"   • Focus on zipcodes/vehicle types with worst loss ratios\")\n",
    "print()\n",
    "\n",
    "print(\"4. IMPLEMENTATION RECOMMENDATIONS:\")\n",
    "print(\"   a. PHASED APPROACH:\")\n",
    "print(\"      • Start with 5-10% premium adjustments\")\n",
    "print(\"      • Monitor impact on portfolio profitability\")\n",
    "print(\"      • Adjust based on actual results\")\n",
    "print()\n",
    "print(\"   b. SEGMENT-SPECIFIC STRATEGY:\")\n",
    "print(\"      • Different adjustments by province, zipcode, vehicle type\")\n",
    "print(\"      • Use model feature importances to guide strategy\")\n",
    "print()\n",
    "print(\"   c. CONTINUOUS MONITORING:\")\n",
    "print(\"      • Update models quarterly with new data\")\n",
    "print(\"      • Track actual vs predicted loss ratios\")\n",
    "print(\"      • Refine models based on performance\")\n",
    "print()\n",
    "\n",
    "print(\"5. EXPECTED BUSINESS IMPACT:\")\n",
    "print(\"   • Improved portfolio profitability\")\n",
    "print(\"   • More competitive pricing for low-risk segments\")\n",
    "print(\"   • Better risk selection and pricing accuracy\")\n",
    "print(\"   • Data-driven decision making\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAVING MODEL RESULTS AND PREDICTIONS\n",
      "============================================================\n",
      "Predictions saved to: data/processed/optimal_premium_predictions.csv\n",
      "Rows: 618,176\n",
      "Columns: 14\n",
      "\n",
      "File includes:\n",
      "  • UnderwrittenCoverID\n",
      "  • PolicyID\n",
      "  • PostalCode\n",
      "  • Province\n",
      "  • VehicleType\n",
      "  • CoverType\n",
      "  • SumInsured\n",
      "  • TotalPremium\n",
      "  • TotalClaims\n",
      "  • LossRatio\n",
      "  • OptimalPremium\n",
      "  • PremiumDifference\n",
      "  • PremiumChangePct\n",
      "  • Profitable\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Save Model Results and Predictions\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING MODEL RESULTS AND PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save predictions to CSV for business use\n",
    "output_cols = [\n",
    "    'UnderwrittenCoverID', 'PolicyID', 'PostalCode', 'Province',\n",
    "    'VehicleType', 'CoverType', 'SumInsured',\n",
    "    'TotalPremium', 'TotalClaims', 'LossRatio',\n",
    "    'OptimalPremium', 'PremiumDifference', 'PremiumChangePct', 'Profitable'\n",
    "]\n",
    "\n",
    "# Only save columns that exist\n",
    "existing_cols = [col for col in output_cols if col in df_premium.columns]\n",
    "\n",
    "# Create reports directory if not exists\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "df_premium[existing_cols].to_csv('../data/processed/optimal_premium_predictions.csv', index=False)\n",
    "\n",
    "print(f\"Predictions saved to: data/processed/optimal_premium_predictions.csv\")\n",
    "print(f\"Rows: {len(df_premium):,}\")\n",
    "print(f\"Columns: {len(existing_cols)}\")\n",
    "print(\"\\nFile includes:\")\n",
    "for col in existing_cols:\n",
    "    print(f\"  • {col}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
